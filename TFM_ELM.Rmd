---
title: "Aplicación de técnicas de machine learning para la identificación de crisis epilépticas de origen metabólico"
author: "Esperanza López Merino"
date: "2026-01-05"
output:
    html_document:
      toc: yes
      toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache=TRUE)
```


```{r, include=FALSE}
library(tidyverse)
library(caret)
library(randomForest)
library(ggplot2)
library(knitr)
library(kableExtra)
library(e1071)
library(kernlab)
library(C50)
library(pROC)
library(iml)
library(ranger)

```

# Cargamos los datos

```{r, warning=FALSE}
data <- read_csv("epilepsy.csv")
```

# Dataset

Creamos la variable enfermedad metabólica (categórica)

```{r}
metab_patients <- read.csv("metabolic_patients.csv")
data2 <- data %>%
  mutate(
    metab_dis = ifelse(subject_id %in% metab_patients$subject_id, "yes", "no"),
    metab_dis = factor(metab_dis, levels = c("no", "yes")))
table(data2$metab_dis)
```


Generamos una nueva variable para contabilizar los ingresos previos por crisis epilépticas.

```{r}
data2 <- data2 %>%
  group_by(subject_id) %>%
  arrange(admittime, .by_group = TRUE) %>%   
  mutate(previous_admissions = row_number()) %>%             
  ungroup()
```


Recodificamos las variables categóricas 

```{r}
data2 <- data2
data2$gender <- as.factor(data2$gender)
data2$race <- as.factor(data2$race)
data2 <- data2 %>% mutate(ICU = if_else(n_icu_stays > 0, 1, 0))
data2$ICU <- as.factor(data2$ICU)

summary(data2)
```


Depurar NAs

Variables
```{r}
threshold <- 0.2
data2 <- data2 %>%
  select(where(~ mean(is.na(.)) <= threshold))
```

Registros

```{r}
data2 <- data2 %>%
  filter(rowMeans(is.na(.)) <= threshold)

#pacientes con múltiples datos anómalos
data2 <- data2 %>%
  filter(hadm_id != 29319545)
data2 <- data2 %>%
  filter(hadm_id != 23586761)
data2 <- data2 %>%
  filter(hadm_id != 25764839)


```

 reducir dimensionalidad raza

```{r}
prop.table(table(data2$race))

data2$race_group <- case_when(
  data2$race %in% c("WHITE", "WHITE - BRAZILIAN", "WHITE - EASTERN EUROPEAN", "WHITE - OTHER EUROPEAN", "WHITE - RUSSIAN", "PORTUGUESE") ~ "White",
  data2$race %in% c("BLACK/AFRICAN", "BLACK/AFRICAN AMERICAN", "BLACK/CAPE VERDEAN", "BLACK/CARIBBEAN ISLAND") ~ "Black",
  data2$race %in% c("ASIAN", "ASIAN - ASIAN INDIAN", "ASIAN - CHINESE", "ASIAN - KOREAN", "ASIAN - SOUTH EAST ASIAN") ~ "Asian",
  data2$race %in% c("HISPANIC OR LATINO", "HISPANIC/LATINO - CENTRAL AMERICAN", "HISPANIC/LATINO - COLUMBIAN",
                    "HISPANIC/LATINO - CUBAN", "HISPANIC/LATINO - DOMINICAN", "HISPANIC/LATINO - GUATEMALAN",
                    "HISPANIC/LATINO - HONDURAN", "HISPANIC/LATINO - MEXICAN", "HISPANIC/LATINO - PUERTO RICAN",
                    "HISPANIC/LATINO - SALVADORAN", "SOUTH_AMERICAN") ~ "Latino",
  data2$race %in% c("AMERICAN INDIAN/ALASKA NATIVE", "NATIVE HAWAIIAN OR OTHER PACIFIC ISLANDER") ~ "Native_Pacific",
  TRUE ~ "Other_Unknown" )
table(data2$race_group)
data2$race_group <- as.factor(data2$race_group)

```



# Imputar NAs y eliminar variables no informativas

```{r}
# Eliminamos columnas no numéricas que no vamos a usar directamente y la variable raza sin agrupar
data_model <- data2 %>% select(-subject_id, -hadm_id, -race, -icd_code, -treatments, -diagnosis_description, -admittime)
summary(data_model)
na_f <- sum(is.na(data_model))
na_0 <- sum(is.na(data))
100*na_f/na_0

# imputar NA con medianas 
num_cols <- names(data_model)[sapply(data_model, is.numeric)]
data_model[num_cols] <- lapply(data_model[num_cols], function(x) ifelse(is.na(x), median(x, na.rm = TRUE), x))
summary(data_model)
```

# Descriptiva por grupos

```{r}
table(data_model$metab_dis)
table(data_model$gender, data_model$metab_dis)
table(data_model$race_group, data_model$metab_dis)
table(data_model$ICU, data_model$metab_dis)

```


```{r}
data_model %>%
  select(where(is.numeric), metab_dis) %>%
  pivot_longer(
    cols = where(is.numeric),
    names_to = "variable",
    values_to = "valor"
  ) %>%
  ggplot(aes(x = metab_dis, y = valor, fill = metab_dis)) +
  geom_boxplot(na.rm = TRUE) +
  facet_wrap(~ variable, scales = "free", ncol = 4) +
  theme_minimal() +
  theme(legend.position = "none")
ggsave(
  "boxplots_variables_numericas.jpg",
  width = 10,
  height = 16,
  dpi = 300
)

```
```{r}
numeric_summary <- data_model %>%
  group_by(metab_dis) %>%
  summarise(
    across(
      .cols = where(is.numeric),
      .fns = list(
        mean = ~ mean(.x, na.rm = TRUE),
        sd   = ~ sd(.x, na.rm = TRUE)
      ),
      .names = "{.col}_{.fn}"
    ))
t(numeric_summary)
```
```{r}
cor(data_model[,sapply(data_model, is.numeric)])
```


```{r}
ggplot(data_model, aes(x = age_at_admission, fill = metab_dis)) +
  geom_histogram(bins = 30, alpha = 0.7, color = "black") +
  facet_wrap(
    ~ metab_dis,
    labeller = labeller(
      metab_dis = c("no" = "Control", "yes" = "Enfermedad Metabólica")
    )
  ) +
  labs(
    title = "Distribución de edad",
    x = "Edad al ingreso",
    y = "Frecuencia",
    fill = "Enfermedad metabólica"
  ) +
  scale_fill_manual(values = c("no" = "#E15759", "yes" = "#4DB" )) +
  theme_minimal() +
  theme(legend.position = "none")
ggsave(
  "histogramas_edad.jpg",
  width = 10,
  height = 6,
  dpi = 300
)
```


```{r}
ggplot(data_model, aes(x = metab_dis, y = glucose, fill = metab_dis)) +
  geom_violin(trim = FALSE, alpha = 0.5) +
  geom_jitter(width = 0.2, alpha = 0.01, color = "black") +
  coord_cartesian(ylim = c(0, 500)) +
  scale_x_discrete(labels = c("no" = "Control", "yes" = "Enfermedad Metabólica"))+
  scale_fill_manual(values = c("no" = "#E15759", "yes" = "#4DB")) +
  labs(title = "Distribución de glucosa por grupo",
       x = "Grupo",
       y = "Glucosa (mg/dL)") +
  theme_minimal() +
  theme(legend.position = "none")
```
```{r}
ggplot(data_model, aes(x = metab_dis, y = urea, fill = metab_dis)) +
  geom_violin(trim = FALSE, alpha = 0.5) +
  geom_jitter(width = 0.2, alpha = 0.01, color = "black") +
  coord_cartesian(ylim = c(0, 100)) +
  scale_x_discrete(labels = c("no" = "Control", "yes" = "Enfermedad Metabólica"))+
  scale_fill_manual(values = c("no" = "#E15759", "yes" = "#4DB")) +
  labs(title = "Distribución de urea por grupo",
       x = "Grupo",
       y = "Urea (mg/dL)") +
  theme_minimal() +
  theme(legend.position = "none")
```
```{r}
ggplot(data_model, aes(x = metab_dis, y = creatinine, fill = metab_dis)) +
  geom_violin(trim = FALSE, alpha = 0.5) +
  geom_jitter(width = 0.2, alpha = 0.01, color = "black") +
  coord_cartesian(ylim = c(0, 5)) +
  scale_x_discrete(labels = c("no" = "Control", "yes" = "Enfermedad Metabólica"))+
  scale_fill_manual(values = c("no" = "#E15759", "yes" = "#4DB")) +
  labs(title = "Distribución de creatinina por grupo",
       x = "Grupo",
       y = "Creatinina (mg/dL)") +
  theme_minimal() +
  theme(legend.position = "none")
```


# outliers 

```{r}
outliers_por_grupo <- data_model %>%
  select(metab_dis, where(is.numeric)) %>%
  group_by(metab_dis) %>%
  summarise(across(
    where(is.numeric),
    ~ sum(
      . < quantile(., 0.25, na.rm = TRUE) - 1.5 * IQR(., na.rm = TRUE) |
      . > quantile(., 0.75, na.rm = TRUE) + 1.5 * IQR(., na.rm = TRUE),
      na.rm = TRUE
    ),
    .names = "outliers_{.col}"
  ))
outliers_por_grupo 
```


```{r}
numeric_data <- data_model %>%
  select(where(is.numeric))

outlier_matrix <- sapply(numeric_data, function(x) {
  q1 <- quantile(x, 0.25, na.rm = TRUE)
  q3 <- quantile(x, 0.75, na.rm = TRUE)
  iqr <- IQR(x, na.rm = TRUE)
  
  x < (q1 - 1.5 * iqr) | x > (q3 + 1.5 * iqr)
})

outliers_per_row <- rowSums(outlier_matrix, na.rm = TRUE)

ggplot(data.frame(outliers = outliers_per_row),
       aes(x = outliers)) +
  geom_histogram(binwidth = 1, fill = "steelblue", color = "black") +
  labs(
    title = "Distribución del número de outliers por muestra",
    x = "Número de variables outlier",
    y = "Número de muestras"
  ) +
  theme_minimal()


```

# Datasets Train y test

```{r}
set.seed(123)

train_index <- createDataPartition(data_model$metab_dis, p = 0.7, list = FALSE)
train <- data_model[train_index, ]
test  <- data_model[-train_index, ]

prop.table(table(train$metab_dis))
prop.table(table(test$metab_dis))

```

# validación cruzada

```{r}
ctrl <- trainControl(
  method = "cv",
  number = 5,
  classProbs = TRUE,
  summaryFunction = twoClassSummary,
  savePredictions = TRUE
)

```


# knn

```{r}

set.seed(123)
knn_grid <- expand.grid(k = c(5, 7, 9, 11))
modelo_knn <- train(
  metab_dis ~ .,
  data = train,
  method = "knn",
  metric = "ROC",
  tuneGrid = knn_grid,
  trControl = ctrl,
  preProcess = c("center", "scale")
)

modelo_knn
```

```{r}
pred_class_knn <- predict(modelo_knn, newdata = test)
pred_prob_knn  <- predict(modelo_knn, newdata = test, type = "prob")[,"yes"]
cf_knn <- confusionMatrix(pred_class_knn, test$metab_dis, positive = "yes")
cf_knn
roc_knn <- roc(test$metab_dis, pred_prob_knn)
auc(roc_knn)
precision_knn <- as.numeric(cf_knn$byClass["Precision"])
recall_knn    <- as.numeric(cf_knn$byClass["Recall"])
F1_knn <- 2 * (precision_knn * recall_knn) / (precision_knn + recall_knn)
F1_knn
```

# Naive Bayes

## laplace = 0

```{r, warning=FALSE}
set.seed(123)

nb_grid0 <- expand.grid(
  fL = 0,        # Laplace = 0
  usekernel = c(TRUE, FALSE),   
  adjust = c(1, 1.5, 2)         
)

modelo_nb0 <- caret::train(
  metab_dis ~ .,
  data = train,
  method = "nb",
  metric = "ROC",
  tuneGrid = nb_grid0,
  trControl = ctrl,
  preProcess = c("center", "scale")
)

modelo_nb0
```

```{r, warning=FALSE}
pred_class_nb0 <- predict(modelo_nb0, newdata = test)
pred_prob_nb0  <- predict(modelo_nb0, newdata = test, type = "prob")[,"yes"]

cf_nb0 <- confusionMatrix(pred_class_nb0, test$metab_dis, positive = "yes")
cf_nb0
roc_nb0 <- roc(test$metab_dis, pred_prob_nb0)
auc(roc_nb0)
precision_nb0 <- as.numeric(cf_nb0$byClass["Precision"])
recall_nb0    <- as.numeric(cf_nb0$byClass["Recall"])
F1_nb0 <- 2 * (precision_nb0 * recall_nb0) / (precision_nb0 + recall_nb0)
F1_nb0
```


## laplace 1

```{r, warning=FALSE}
set.seed(123)

nb_grid1 <- expand.grid(
  fL = 1,                       # Laplace = 1
  usekernel = c(TRUE, FALSE),
  adjust = c(1, 1.5, 2)
)

modelo_nb1 <- caret::train(
  metab_dis ~ .,
  data = train,
  method = "nb",
  metric = "ROC",
  tuneGrid = nb_grid1,
  trControl = ctrl,
  preProcess = c("center", "scale")
)

modelo_nb1
```

```{r, warning=FALSE}
pred_class_nb1 <- predict(modelo_nb1, newdata = test)
pred_prob_nb1  <- predict(modelo_nb1, newdata = test, type = "prob")[,"yes"]

cf_nb1 <- confusionMatrix(pred_class_nb1, test$metab_dis, positive = "yes")
cf_nb1

roc_nb1 <- roc(test$metab_dis, pred_prob_nb1)
auc(roc_nb1)
precision_nb1 <- as.numeric(cf_nb1$byClass["Precision"])
recall_nb1 <- as.numeric(cf_nb1$byClass["Recall"])
F1_nb1 <- 2 * (precision_nb1 * recall_nb1) / (precision_nb1 + recall_nb1)
F1_nb1
```


# SVM

## Lineal

```{r, warning=FALSE,  message=FALSE}
set.seed(123)

grid_linear <- expand.grid(
  C = c(0.01, 0.1, 1, 5)
)

svm_linear <- train(
  metab_dis ~ .,
  data = train,
  method = "svmLinear",
  metric = "ROC",
  tuneGrid = grid_linear,
  trControl = ctrl,
  preProcess = c("center", "scale")
)

svm_linear

```


```{r}
pred_class_lin <- predict(svm_linear, newdata = test)
pred_prob_lin  <- predict(svm_linear, newdata = test, type = "prob")[,"yes"]

cf_lin <- confusionMatrix(pred_class_lin, test$metab_dis, positive = "yes")
cf_lin
roc_lin <- roc(test$metab_dis, pred_prob_lin)
auc(roc_lin)
precision_lin <- as.numeric(cf_lin$byClass["Precision"])
recall_lin<- as.numeric(cf_lin$byClass["Recall"])
F1_lin <- 2 * (precision_lin * recall_lin) / (precision_lin + recall_lin)
F1_lin
```

## Radial

```{r, warning=FALSE, message=FALSE}
set.seed(123)

grid_svm_rad <- expand.grid(
  sigma = c(0.1),
  C     = c(1)
)

svm_rad <- train(
  metab_dis ~ .,
  data = train,
  method = "svmRadial",
  metric = "ROC",
  tuneGrid = grid_svm_rad,
  trControl = ctrl,
  preProcess = c("center", "scale")
)

svm_rad

```

```{r}
pred_class_svm_rad <- predict(svm_rad, test)
pred_prob_svm_rad  <- predict(svm_rad, test, type="prob")[,"yes"]

cf_svm_rad <- confusionMatrix(pred_class_svm_rad, test$metab_dis, positive="yes")
cf_svm_rad

roc_svm_rad <- roc(test$metab_dis, pred_prob_svm_rad)
auc(roc_svm_rad)
precision_svm_rad <- as.numeric(cf_svm_rad$byClass["Precision"])
recall_svm_rad <- as.numeric(cf_svm_rad$byClass["Recall"])
F1_svm_rad <- 2 * (precision_svm_rad * recall_svm_rad) / (precision_svm_rad + recall_svm_rad)
F1_svm_rad
```


# Random Forest 

## 100 árboles

```{r}
set.seed(123)

grid <- expand.grid(
  mtry = c(3, 5, 7, 10)   
)

modelo_rf_100 <- train(
  metab_dis ~ .,
  data = train,
  method = "rf",          
  trControl = ctrl,       
  tuneGrid = grid,
  metric = "ROC",
  ntree = 100             
)

modelo_rf_100
```

```{r}
pred_class_rf_100 <- predict(modelo_rf_100, test)
pred_prob_rf_100  <- predict(modelo_rf_100, test, type="prob")[,"yes"]

cf_rf_100= confusionMatrix(pred_class_rf_100, test$metab_dis, positive="yes")
cf_rf_100
roc_rf_100 <- roc(test$metab_dis, pred_prob_rf_100)
auc(roc_rf_100)
precision_rf_100 <- as.numeric(cf_rf_100$byClass["Precision"])
recall_rf_100 <- as.numeric(cf_rf_100$byClass["Recall"])
F1_rf_100 <- 2 * (precision_rf_100 * recall_rf_100) / (precision_rf_100 + recall_rf_100)
F1_rf_100
```

## 500 árboles

```{r}
set.seed(123)

grid <- expand.grid(
  mtry = c(3, 5, 7, 10)   
)

modelo_rf_500 <- train(
  metab_dis ~ .,
  data = train,
  method = "rf",          
  trControl = ctrl,       
  tuneGrid = grid,
  metric = "ROC",
  ntree = 500             
)

modelo_rf_500
```

```{r}
pred_class_rf_500 <- predict(modelo_rf_500, test)
pred_prob_rf_500  <- predict(modelo_rf_500, test, type="prob")[,"yes"]

cf_rf_500= confusionMatrix(pred_class_rf_500, test$metab_dis, positive="yes")
cf_rf_500
roc_rf_500 <- roc(test$metab_dis, pred_prob_rf_500)
auc(roc_rf_500)
precision_rf_500 <- as.numeric(cf_rf_500$byClass["Precision"])
recall_rf_500 <- as.numeric(cf_rf_500$byClass["Recall"])
F1_rf_500 <- 2 * (precision_rf_500 * recall_rf_500) / (precision_rf_500 + recall_rf_500)
F1_rf_500
```
## 1000 árboles

```{r}
set.seed(123)

modelo_rf_1000 <- train(
  metab_dis ~ .,
  data = train,
  method = "rf",          
  trControl = ctrl,       
  tuneGrid = grid,
  metric = "ROC",
  ntree = 1000             
)

modelo_rf_1000
```
```{r}
pred_class_rf_1000 <- predict(modelo_rf_1000, test)
pred_prob_rf_1000  <- predict(modelo_rf_1000, test, type="prob")[,"yes"]

cf_rf_1000= confusionMatrix(pred_class_rf_1000, test$metab_dis, positive="yes")
cf_rf_1000
roc_rf_1000 <- roc(test$metab_dis, pred_prob_rf_1000)
auc(roc_rf_1000)
precision_rf_1000 <- as.numeric(cf_rf_1000$byClass["Precision"])
recall_rf_1000 <- as.numeric(cf_rf_1000$byClass["Recall"])
F1_rf_1000 <- 2 * (precision_rf_1000 * recall_rf_1000) / (precision_rf_1000 + recall_rf_1000)
F1_rf_1000
```

# XGBoost

```{r}
set.seed(123)

xgb_grid <- expand.grid(
  nrounds = 100,
  max_depth = c(3, 5, 7, 9),
  eta = c(0.05, 0.1, 0.3, 0.5),
  gamma = 0,
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = 1
)

modelo_xgb <- train(
  metab_dis ~ .,
  data = train,
  method = "xgbTree",
  trControl = ctrl,   
  tuneGrid = xgb_grid,
  metric = "ROC"
)

modelo_xgb

```
```{r}

pred_class_xgb <- predict(modelo_xgb, newdata = test)
pred_prob_xgb  <- predict(modelo_xgb, newdata = test, type = "prob")[, "yes"]

cf_xgb <- confusionMatrix(pred_class_xgb, test$metab_dis, positive = "yes")
cf_xgb
roc_xgb <- roc(test$metab_dis, pred_prob_xgb)
auc(roc_xgb)
precision_xgb <- as.numeric(cf_xgb$byClass["Precision"])
recall_xgb<- as.numeric(cf_xgb$byClass["Recall"])
F1_xgb <- 2 * (precision_xgb * recall_xgb) / (precision_xgb + recall_xgb)
F1_xgb
```



# logreg

```{r}
modelo_logit <- caret::train(
  metab_dis ~ .,
  data = train,
  method = "glm",
  family = binomial,
  trControl = ctrl,
  metric = "ROC",
  preProcess = c("center", "scale")
)
summary(modelo_logit$finalModel)
modelo_logit
```

```{r}

pred_class_log <- predict(modelo_logit, newdata = test)
pred_prob_log  <- predict(modelo_logit, newdata = test, type = "prob")[, "yes"]

cf_log <- confusionMatrix(pred_class_log, test$metab_dis, positive = "yes")
cf_log

roc_log <- roc(test$metab_dis, pred_prob_log)
auc(roc_log)
precision_log <- as.numeric(cf_log$byClass["Precision"])
recall_log<- as.numeric(cf_log$byClass["Recall"])
F1_log <- 2 * (precision_log * recall_log) / (precision_log + recall_log)
F1_log
```

# Curvas ROC


```{r}
plot(
  roc_knn,
  col = "#2CA1F9", 
  lwd = 2,
  legacy.axes = TRUE,
  main = "Curvas ROC de los modelos evaluados"
)
plot(roc_nb0, col = "#E15759", lwd = 2, add = TRUE)     
plot(roc_nb1, col = "#A9A9A9", lwd = 2, add = TRUE)     
plot(roc_lin, col = "#78E08F", lwd = 2, add = TRUE)     
plot(roc_svm_rad, col = "#EDC948", lwd = 2, add = TRUE) 
plot(roc_rf_100, col = "#000000", lwd = 2, add = TRUE)   
plot(roc_rf_500, col = "#E85C00", lwd = 2, add = TRUE)      
plot(roc_rf_1000, col = "#59A14F", lwd = 2, add = TRUE) 
plot(roc_xgb, col = "#63B4FF", lwd = 2, add = TRUE)     
plot(roc_log, col = "#FF9F00", lwd = 2, add = TRUE)

legend(
  "bottomright",
  legend = c(
    paste0("KNN (AUC=", round(auc(roc_knn), 3), ")"),
    paste0("Naive Bayes L0 (AUC=", round(auc(roc_nb0), 3), ")"),
    paste0("Naive Bayes L1 (AUC=", round(auc(roc_nb1), 3), ")"),
    paste0("SVM lineal (AUC=", round(auc(roc_lin), 3), ")"),
    paste0("SVM radial (AUC=", round(auc(roc_svm_rad), 3), ")"),
    paste0("Random Forest 100 (AUC=", round(auc(roc_rf_100), 3), ")"),
    paste0("Random Forest 500 (AUC=", round(auc(roc_rf_500), 3), ")"),
    paste0("Random Forest 1000 (AUC=", round(auc(roc_rf_1000), 3), ")"),
    paste0("XGBoost (AUC=", round(auc(roc_xgb), 3), ")"),
    paste0("Regresión logística (AUC=", round(auc(roc_log), 3), ")")
  ),
  col = c("#2CA1F9","#E15759","#A9A9A9","#78E08F","#EDC948", "#000000", "#E85C00","#59A14F","#63B4FF","#FF9F00"),
  lwd = 2,
  cex = 0.8
)
```


# Interpretabilidad

## knn
```{r}
predictor_knn <- Predictor$new(
  model = modelo_knn,      
  data  = train[, setdiff(names(train), "metab_dis")],
  y     = train$metab_dis,
  type  = "prob" 
)
imp_knn <- FeatureImp$new(
  predictor_knn,
  loss = "ce"   
)
plot(imp_knn)
```


## SVM_lin
```{r}
predictor_lin <- Predictor$new(
  model = svm_linear,      
  data  = train[, setdiff(names(train), "metab_dis")],
  y     = train$metab_dis
)
imp_svm_lin <- FeatureImp$new(
  predictor_lin,
  loss = "ce"   
)
plot(imp_svm_lin)
```

## SVM_RAD
```{r}
predictor_svm_rad <- Predictor$new(
  model = svm_rad,      
  data  = train[, setdiff(names(train), "metab_dis")],
  y     = train$metab_dis
)
imp_svm_rad <- FeatureImp$new(
  predictor_svm_rad,
  loss = "ce"   
)
plot(imp_svm_rad)
```




## RF_100


```{r}
predictor_rf <- Predictor$new(
  model = modelo_rf_100,
  data  = train[, setdiff(names(data_model), "metab_dis")],
  y     = train$metab_dis
)
imp_rf <- FeatureImp$new(
  predictor_rf,
  loss = "ce" 
)
plot(imp_rf)
```
 
## XGBoost

```{r}
predictor_xgb <- Predictor$new(
  model = modelo_xgb,     
  data  = train[, setdiff(names(train), "metab_dis")],
  y     = train$metab_dis
)
imp_xgb <- FeatureImp$new(
  predictor_xgb,
  loss = "ce"   
)
plot(imp_xgb)
```

## logreg

```{r}
predictor_log <- Predictor$new(
  model = modelo_logit,      
  data  = train[, setdiff(names(train), "metab_dis")],
  y     = train$metab_dis
)
imp_log <- FeatureImp$new(
  predictor_log,
  loss = "ce"   
)
plot(imp_log)
```

